{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import codecs\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_files('./sampleDATA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = []\n",
    "rec = []\n",
    "sci = []\n",
    "talk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(path):\n",
    "    lt=[]\n",
    "    w1 = codecs.open(path,encoding='utf-8', errors='ignore').read()\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'[a-z,A-Z]{2,}')\n",
    "    token=tokenizer.tokenize(w1)\n",
    "    stemmer = EnglishStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in token:\n",
    "        w = w.translate(str.maketrans('','',string.punctuation))\n",
    "        w = stemmer.stem(w)\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        if w.lower() not in stopWords and w.lower() not in lt:\n",
    "            lt.append(w)\n",
    "    return list(set(lt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files(file_paths):\n",
    "    \n",
    "    for path in file_paths:\n",
    "        if \"comp\" in path:\n",
    "            comp.append(clean_files(path))\n",
    "        elif \"rec\" in path:\n",
    "            rec.append(clean_files(path))\n",
    "        elif \"sci\" in path:\n",
    "            sci.append(clean_files(path))\n",
    "        elif \"talk\" in path:\n",
    "            talk.append(clean_files(path))\n",
    "    return comp, rec, sci,talk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data, rec_data, sci_data, talk_data = open_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(comp))\n",
    "print(len(rec))\n",
    "print(len(sci))\n",
    "print(len(talk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train,comp_test = train_test_split(comp, train_size=0.7,test_size=0.3)\n",
    "rec_train, rec_test = train_test_split(rec, train_size=0.7,test_size=0.3)\n",
    "sci_train, sci_test = train_test_split(sci, train_size=0.7,test_size=0.3)\n",
    "talk_train,talk_test = train_test_split(talk, train_size=0.7,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(comp_train))\n",
    "print(len(sci_train))\n",
    "print(len(rec_train))\n",
    "print(len(talk_train))\n",
    "print(len(comp_test))\n",
    "print(len(sci_test))\n",
    "print(len(rec_test))\n",
    "print(len(talk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[comp_train,rec_train,sci_train,talk_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryList=['compTrain','recTrain','sciTrain','talkTrain','compTest','recTest','sciTest','talkTest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_word_set(data_list):\n",
    "    words = []\n",
    "    word_set=[]\n",
    "    for docs in data_list:\n",
    "        for doc in docs:\n",
    "            for word in doc:\n",
    "                words.append(word)\n",
    "            word_set += [word for word in words if word.isalpha()]\n",
    "    return list(set(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=get_unique_word_set(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(datalist,dataset,unique_words):   \n",
    "    vectorizer = CountVectorizer(analyzer='word',vocabulary=unique_words)\n",
    "    data=[\"\".join(x) for x in datalist]\n",
    "    trans = vectorizer.fit_transform(data)\n",
    "    features=vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(data=trans.toarray(), columns = features)\n",
    "    df.to_csv(dataset+'_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_csv(comp_train,categoryList[0],unique_words)\n",
    "convert_csv(comp_test,categoryList[1],unique_words)\n",
    "convert_csv(sci_train,categoryList[2],unique_words)\n",
    "convert_csv(sci_train,categoryList[3],unique_words)\n",
    "convert_csv(rec_train,categoryList[4],unique_words)\n",
    "convert_csv(rec_train,categoryList[5],unique_words)\n",
    "convert_csv(talk_train,categoryList[6],unique_words)\n",
    "convert_csv(talk_train,categoryList[7],unique_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
